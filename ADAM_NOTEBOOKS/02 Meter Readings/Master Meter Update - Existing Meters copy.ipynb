{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1936159e",
   "metadata": {},
   "source": [
    "# Master Meter Update Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a903a",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca338f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_23004\\2875485900.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Imported\n",
      "Starting and ending indices set, processing meters 0 to 99\n",
      "SQL Connection String Created\n",
      "EyeDro Endpoint and Key Set\n",
      "List of existing meter serials in SQL database gathered 568\n",
      "function created: eyedro_getdata\n",
      "function created: parse_timestamp\n",
      "function created: fill_missing_timestamps\n",
      "function created: impute_and_summarize\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json as js\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime as dt, timedelta, timezone\n",
    "import pytz  # for timezone handling\n",
    "print('Libraries Imported')\n",
    "\n",
    "# Set Global Variables\n",
    "S_IDX = 0\n",
    "E_IDX = 100\n",
    "print(f'Starting and ending indices set, processing meters {S_IDX} to {E_IDX -1}')\n",
    "\n",
    "ENGINE = 'postgresql://avnadmin:AVNS_zSrniBsHGVQSqhqunlJ@pg-unhcr-unhcr-007.a.aivencloud.com:15602/defaultdb' ####smh create_engine('postgresql://postgres:4raxeGo5xgB$@localhost:5432/eyedro_meters')\n",
    "print('SQL Connection String Created')\n",
    "\n",
    "API_BASE_URL = \"https://api.eyedro.com/customcmd\"\n",
    "USER_KEY = \"UNHCRMHiYgbHda9cRv4DuPp28DnAnfeV8s6umP5R\"\n",
    "USER_KEY_GET_DATA = \"UNHCRp28DnAV8s6uHdMHiYgba95RcRv4DnfeuPmP\"\n",
    "print('EyeDro Endpoint and Key Set')\n",
    "\n",
    "\n",
    "# Reference the below view which calls the SQL database to list the table names for the meter tables\n",
    "# This is our list of meters which already have data which we will then update\n",
    "\n",
    "'''\n",
    "create or replace view vw_table_list\n",
    "as \n",
    "select \"table_name\"\n",
    "from information_schema.tables\n",
    "where \"table_catalog\" = 'eyedro_meters'\n",
    "and \"table_name\" like '009%'\n",
    "and \"table_type\" = 'BASE TABLE'\n",
    ";\n",
    "'''\n",
    "#smh\n",
    "#L_SQL_SERIALS = pd.read_sql_query(\"select * from vw_table_list;\",con=ENGINE).serial_num.to_list()\n",
    "try:\n",
    "    L_SQL_SERIALS = set(pd.read_sql_query(f\"SELECT table_name FROM gb_2024.vw_table_list;\",con=ENGINE).table_name.values)\n",
    "except Exception as e:\n",
    "    print('EEEEEEEEEEEEEE',e)\n",
    "    pass\n",
    "print('List of existing meter serials in SQL database gathered', len(L_SQL_SERIALS))\n",
    "\n",
    "# Define functions for various operations in the script\n",
    "\n",
    "# Function to convert Timestamp to epoch time\n",
    "# def pd_timestamp_to_epoch(timestamp):\n",
    "#     return timestamp.timestamp()\n",
    "\n",
    "def eyedro_getdata(serial, timestamp):\n",
    "\n",
    "    '''\n",
    "    This function takes as its input a meter serial number and an epoch timestamp and calls the GetData API to \n",
    "    retrieve the prior day's readings (96 steps at 15-minute intervals). It returns the response as JSON text\n",
    "    '''\n",
    "    meter_url = \"https://api.eyedro.com/customcmd?Cmd=Unhcr.GetData&DeviceSerial=\" + str(serial) + \"&DateStartSecUtc=\" + str(timestamp) + f\"&DateNumSteps=96&UserKey={USER_KEY_GET_DATA}\"\n",
    "    response = requests.get(meter_url, timeout=600)\n",
    "    return js.loads(response.text)\n",
    "\n",
    "print('function created: eyedro_getdata')\n",
    "\n",
    "def parse_timestamp(timestamp):\n",
    "    '''\n",
    "    Function to parse out date and time information from timestamp for later use (feature engineering)\n",
    "    '''\n",
    "    ts = dt.utcfromtimestamp(timestamp).replace(tzinfo=pytz.utc)\n",
    "\n",
    "    # Extract various components\n",
    "    gmt_timestamp = ts.isoformat()\n",
    "    year = ts.year\n",
    "    month = ts.month\n",
    "    week = ts.isocalendar()[1]  # Week number of the year\n",
    "    day_of_month = ts.day\n",
    "    day_of_week = ts.strftime('%A').lower()  # Full weekday name in lowercase\n",
    "    hour = ts.hour\n",
    "    minute = ts.minute\n",
    "    time = ts.strftime('%H:%M')\n",
    "\n",
    "    return {\n",
    "        'gmt_timestamp': gmt_timestamp,\n",
    "        'year': year,\n",
    "        'month': month,\n",
    "        'week': week,\n",
    "        'day_of_month': day_of_month,\n",
    "        'day_of_week': day_of_week,\n",
    "        'hour': hour,\n",
    "        'minute': minute,\n",
    "        'time': time\n",
    "    }\n",
    "\n",
    "print('function created: parse_timestamp')\n",
    "\n",
    "def fill_missing_timestamps(df):\n",
    "    '''\n",
    "    Function to scan for missing timestamps and synthetically create 0-value Wh readings to fill these gaps\n",
    "    '''\n",
    "    # Convert the \"Timestamp\" column to a datetime object\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
    "\n",
    "\n",
    "    # Find the minimum and maximum timestamps in the dataframe\n",
    "    min_timestamp = df['Timestamp'].min()\n",
    "    max_timestamp = df['Timestamp'].max()\n",
    "\n",
    "    # Generate a list of expected timestamps at 15-minute intervals\n",
    "    expected_timestamps = pd.date_range(start=min_timestamp, end=max_timestamp, freq='15min')\n",
    "\n",
    "    # Identify missing timestamps\n",
    "    missing_timestamps = expected_timestamps[~expected_timestamps.isin(df['Timestamp'])]\n",
    "\n",
    "    # Create new rows for missing timestamps with 0 in the \"Wh\" column\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Timestamp': missing_timestamps,\n",
    "        'DeviceSerial': df['DeviceSerial'].iloc[0],  # Assuming all rows have the same serial number\n",
    "        'Wh': 0\n",
    "    })\n",
    "\n",
    "    # Concatenate the missing data with the original dataframe\n",
    "    df = pd.concat([df, missing_data])\n",
    "\n",
    "    # Sort the dataframe by timestamp\n",
    "    df.sort_values(by='Timestamp', inplace=True)\n",
    "    \n",
    "    # Convert the timestamp back to epoch format\n",
    "    #smh df['Timestamp'] = df['Timestamp'].astype('Int64') // 10**9\n",
    "    df['Timestamp'] = df['Timestamp'].astype(np.int64) // 10**9\n",
    "    #df['Timestamp'] = df['Timestamp'].apply(lambda x: int(x.timestamp())) ######.apply(pd_timestamp_to_epoch).astype('int32')\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "print('function created: fill_missing_timestamps')\n",
    "\n",
    "def impute_and_summarize(df):\n",
    "    # Calculate the mean Wh value for each timeslot of each day of the week (only non-zero values)\n",
    "    df['timeslot_mean'] = df.groupby(['day_of_week', 'time'])['Wh'].transform(lambda x: x[x > 0].mean())\n",
    "\n",
    "    # Calculate the median Wh value for each timeslot of each day of the week (only non-zero values)\n",
    "    df['timeslot_median'] = df.groupby(['day_of_week', 'time'])['Wh'].transform(lambda x: x[x > 0].median())\n",
    "\n",
    "    # Create \"imputed_mean\" column based on conditions\n",
    "    df['imputed_mean'] = df.apply(lambda row: row['timeslot_mean'] if row['Wh'] == 0 else row['Wh'], axis=1)\n",
    "\n",
    "    # Create \"imputed_median\" column based on conditions\n",
    "    df['imputed_median'] = df.apply(lambda row: row['timeslot_median'] if row['Wh'] == 0 else row['Wh'], axis=1)\n",
    "\n",
    "    # Create a boolean column to indicate when the calculated value was used\n",
    "    df['calculated_used'] = df['Wh'] == 0\n",
    "\n",
    "    return df\n",
    "\n",
    "print('function created: impute_and_summarize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bbb5d",
   "metadata": {},
   "source": [
    "## Conduct full update of meter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1a9a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009004E2 | 0:00:14.911260 elapsed | success | Rows Loaded: 10461  461\n"
     ]
    }
   ],
   "source": [
    "#for serial in L_SQL_SERIALS[S_IDX:E_IDX]:\n",
    "for serial in L_SQL_SERIALS:\n",
    "    \n",
    "    if serial != '009004E2':\n",
    "        continue\n",
    "    rt_st = dt.now()\n",
    "    \n",
    "    try:\n",
    "        # Load the data from the existing table #smh \n",
    "        df_temp = pd.read_sql_query(f'select * from gb_2024.\"{serial}\"',con=ENGINE)\n",
    "        #smh pd.read_sql_query(f'select * from \"{serial}\"',con=ENGINE)\n",
    "\n",
    "        # Trim off columns in preparation to append new data\n",
    "        cols_to_keep = ['DeviceSerial','Timestamp','Wh']\n",
    "        df_temp = df_temp[cols_to_keep]\n",
    "\n",
    "        # Find the maximum timestamp in the \"Timestamp\" column\n",
    "        max_timestamp = df_temp['Timestamp'].max()\n",
    "\n",
    "        # Calculate the current timestamp in UTC for the time when the script is run\n",
    "        current_timestamp = int(dt.now(timezone.utc).timestamp())\n",
    "\n",
    "        # Create a list of midnight timestamps between max_timestamp and current_timestamp to pass to the API call\n",
    "        midnight_timestamps = []\n",
    "        current_date = dt.utcfromtimestamp(max_timestamp).date()\n",
    "        midnight = dt(current_date.year, current_date.month, current_date.day, 0, 0, 0, tzinfo=timezone.utc)\n",
    "\n",
    "        ts = max_timestamp\n",
    "        midnight_timestamps.append(ts)\n",
    "        ts = midnight.timestamp()\n",
    "        ts += 24 * 60 * 60\n",
    "        while ts <= current_timestamp:\n",
    "            midnight_timestamps.append(int(ts))\n",
    "            ts += 24 * 60 * 60\n",
    "            \n",
    "        # while midnight.timestamp() <= current_timestamp:\n",
    "        #     midnight_timestamps.append(int(midnight.timestamp()))\n",
    "        #     midnight += timedelta(days=1)\n",
    "\n",
    "        # Create list to hold responses to the API calls, storing each response as an element in a list\n",
    "        li_responses = []\n",
    "\n",
    "        # Call the API to fetch data, skipping if a fatal error is encountered\n",
    "        for timestamp in midnight_timestamps:\n",
    "            try:\n",
    "                li_responses.append(eyedro_getdata(serial, timestamp))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Prepare an empty list to hold all rows of the final DataFrame\n",
    "        all_rows = []\n",
    "\n",
    "        # Iterate over each response in the list of responses and format into a dataframe\n",
    "        for data in li_responses:\n",
    "            header_info = {\n",
    "                'DeviceSerial': data['DeviceSerial']\n",
    "            }\n",
    "\n",
    "            for reading in data['Data']['Wh'][0]:\n",
    "                timestamp, meter_reading = reading\n",
    "                row = {**header_info, 'Timestamp': timestamp, 'Wh': meter_reading}\n",
    "\n",
    "                # Add the combined information to the list\n",
    "                all_rows.append(row)\n",
    "\n",
    "        # Create a DataFrame from the API response which we will add to the existing data\n",
    "        df_new_data = pd.DataFrame(all_rows)\n",
    "\n",
    "        # Add the newly-fetched meter readings to the existing data\n",
    "        df_updated = pd.concat([df_temp, df_new_data], ignore_index=True)\n",
    "\n",
    "        # Reset the index to create a new sequential index\n",
    "        df_updated = df_updated.reset_index(drop=True)\n",
    "\n",
    "        # Scan for missing 15-minute increments and fill gaps in the data with 0-Wh readings\n",
    "        df_updated = fill_missing_timestamps(df_updated)\n",
    "\n",
    "        # Parse date and time info out of the timestamps\n",
    "        parsed_timestamps = df_updated['Timestamp'].apply(parse_timestamp)\n",
    "\n",
    "        # Add the parsed data back into the DataFrame\n",
    "        df_updated = df_updated.join(pd.json_normalize(parsed_timestamps))\n",
    "\n",
    "        # Sort the DataFrame by 'Wh' in descending order to put non-zero Wh values first (for use later in dropping duplicates)\n",
    "        df_updated = df_updated.sort_values(by=['Wh'], ascending=False)\n",
    "\n",
    "        # Drop duplicates based on 'Timestamp' and keep the first occurrence\n",
    "        df_updated = df_updated.drop_duplicates(subset=['Timestamp'], keep='first')\n",
    "\n",
    "        # Re-sort the dataframe by timestamp\n",
    "        df_updated = df_updated.sort_values(by=['Timestamp'], ascending=True)\n",
    "\n",
    "        # Parse date and time info out of the timestamps\n",
    "        #parsed_timestamps = df_updated['Timestamp'].apply(parse_timestamp)\n",
    "\n",
    "        #df_updated = df_updated[['DeviceSerial','Timestamp','Wh']]\n",
    "        # Add the parsed data back into the DataFrame\n",
    "        #df_updated = df_updated.join(pd.json_normalize(parsed_timestamps))\n",
    "        \n",
    "        # Impute means and medians and create imputed value columns for later use\n",
    "        df_updated = impute_and_summarize(df_updated)\n",
    "\n",
    "        # Load resulting update to SQL\n",
    "        res = df_updated.to_sql(f\"{serial}\", ENGINE, schema='gb_2024',if_exists='replace')\n",
    "        \n",
    "        # Print status message\n",
    "        rt_et = dt.now()\n",
    "        print(f\"{serial} | {rt_et-rt_st} elapsed | success | Rows Loaded: {len(df_updated)}  {res}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        rt_et = dt.now()\n",
    "        \n",
    "        # Capture the exception and print the error message\n",
    "        print(f\"{serial} | {rt_et-rt_st} elapsed | failure | error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16dda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
